{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "682757de",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip pip install lightgbm optuna -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3fff721",
   "metadata": {},
   "source": [
    "# Model 2: Cross-Domain Conversion Prediction\n",
    "\n",
    "## Goal\n",
    "\n",
    "Build a **production-ready pipeline** that predicts whether a user will make a purchase,\n",
    "using behavioral signals from retail, marketplace, and offers domains.\n",
    "\n",
    "## What This Notebook Includes\n",
    "\n",
    "1. **Feature Engineering** - Temporal aggregations, recency, embeddings, brand flags\n",
    "2. **Time-Based Split** - Train/validation/test with proper temporal cutoffs\n",
    "3. **Baseline + Strong Models** - Logistic Regression â†’ LightGBM\n",
    "4. **Experiment Logging** - CSV-based lightweight tracking\n",
    "5. **Hyperparameter Tuning** - Optuna-based Bayesian optimization\n",
    "6. **Comprehensive Evaluation** - PR-AUC, ROC-AUC, calibration, lift, precision@k\n",
    "7. **Error Analysis** - Confusion analysis, feature ablation\n",
    "8. **Model Packaging** - Serialize model + feature pipeline\n",
    "9. **Interactive Demo** - ipywidgets UI for predictions\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Cleaned data in `cleaned_data/`\n",
    "- (Optional) Item embeddings from Model 1 in `models/item_embeddings/`\n",
    "\n",
    "---\n",
    "## README: How to Run This Notebook\n",
    "\n",
    "1. **Install dependencies**: `pip install lightgbm optuna ipywidgets scikit-learn`\n",
    "2. **Run all cells** in order (Kernel â†’ Restart & Run All)\n",
    "3. **Training time**: ~5-10 minutes depending on data size\n",
    "4. **Outputs saved to**: `models/conversion_model/` and `outputs/demo_viz/`\n",
    "5. **Interactive demo**: Run the last section to try predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258304ee",
   "metadata": {},
   "source": [
    "---\n",
    "## Configuration and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f424a7",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Standard library\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import pickle\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "\n",
    "# Data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ML Libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, precision_recall_curve, auc, \n",
    "    confusion_matrix, classification_report,\n",
    "    brier_score_loss, precision_score, recall_score\n",
    ")\n",
    "from sklearn.calibration import calibration_curve\n",
    "\n",
    "# Gradient Boosting\n",
    "import lightgbm as lgb\n",
    "\n",
    "# Hyperparameter tuning\n",
    "try:\n",
    "    import optuna\n",
    "    OPTUNA_AVAILABLE = True\n",
    "except ImportError:\n",
    "    OPTUNA_AVAILABLE = False\n",
    "    print(\"Optuna not installed. Hyperparameter tuning will use random search.\")\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Interactive widgets\n",
    "try:\n",
    "    import ipywidgets as widgets\n",
    "    from IPython.display import display, HTML\n",
    "    WIDGETS_AVAILABLE = True\n",
    "except ImportError:\n",
    "    WIDGETS_AVAILABLE = False\n",
    "    print(\"ipywidgets not available. Interactive demo will be disabled.\")\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING) if OPTUNA_AVAILABLE else None\n",
    "\n",
    "# Configuration\n",
    "CLEANED_DATA_DIR = \"cleaned_data\"\n",
    "EMBEDDINGS_DIR = \"models/item_embeddings\"\n",
    "OUTPUT_DIR = \"models/conversion_model\"\n",
    "DEMO_VIZ_DIR = \"outputs/demo_viz\"\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# Ensure output directories exist\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "os.makedirs(DEMO_VIZ_DIR, exist_ok=True)\n",
    "\n",
    "# Set random seeds\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")\n",
    "print(f\"Demo visualizations: {DEMO_VIZ_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b13bc0",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f241ea83",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def load_all_data() -> Dict[str, pd.DataFrame]:\n",
    "    \"\"\"Load all required datasets.\"\"\"\n",
    "    print(\"Loading datasets...\")\n",
    "    data = {}\n",
    "    \n",
    "    files = {\n",
    "        'users': 'users_clean.parquet',\n",
    "        'retail_events': 'retail_events_clean.parquet',\n",
    "        'marketplace_events': 'marketplace_events_clean.parquet',\n",
    "        'offers_events': 'offers_events_clean.parquet',\n",
    "        'payments_events': 'payments_events_clean.parquet',\n",
    "        'payments_receipts': 'payments_receipts_clean.parquet',\n",
    "    }\n",
    "    \n",
    "    for name, filename in files.items():\n",
    "        path = os.path.join(CLEANED_DATA_DIR, filename)\n",
    "        if os.path.exists(path):\n",
    "            data[name] = pd.read_parquet(path)\n",
    "            print(f\"  {name}: {len(data[name]):,} records\")\n",
    "        else:\n",
    "            print(f\"  Warning: {filename} not found\")\n",
    "            data[name] = None\n",
    "    \n",
    "    # Try to load item embeddings (optional)\n",
    "    emb_path = os.path.join(EMBEDDINGS_DIR, 'item_embeddings.parquet')\n",
    "    if os.path.exists(emb_path):\n",
    "        data['item_embeddings'] = pd.read_parquet(emb_path)\n",
    "        print(f\"  item_embeddings: {len(data['item_embeddings']):,} items\")\n",
    "    else:\n",
    "        print(f\"  item_embeddings: not found (will skip embedding features)\")\n",
    "        data['item_embeddings'] = None\n",
    "    \n",
    "    return data\n",
    "\n",
    "data = load_all_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275d7858",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Define Time-Based Split\n",
    "\n",
    "**Critical**: We must use temporal splits to prevent data leakage.\n",
    "- Features are computed from events BEFORE the cutoff\n",
    "- Labels are derived from payments AFTER the cutoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf1fdd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_temporal_boundaries(payments: pd.DataFrame) -> Dict[str, pd.Timedelta]:\n",
    "    \"\"\"\n",
    "    Define temporal boundaries for train/val/test split.\n",
    "    \n",
    "    Split strategy:\n",
    "    - Train: First 60% of time range\n",
    "    - Validation: Next 20% \n",
    "    - Test: Final 20%\n",
    "    \"\"\"\n",
    "    # Get min/max timestamps\n",
    "    min_ts = payments['timestamp'].min()\n",
    "    max_ts = payments['timestamp'].max()\n",
    "    total_range = max_ts - min_ts\n",
    "    \n",
    "    print(f\"Payment data time range:\")\n",
    "    print(f\"  Start: {min_ts}\")\n",
    "    print(f\"  End: {max_ts}\")\n",
    "    print(f\"  Duration: {total_range}\")\n",
    "    \n",
    "    # Define cutoffs\n",
    "    train_cutoff = min_ts + 0.6 * total_range\n",
    "    val_cutoff = min_ts + 0.8 * total_range\n",
    "    \n",
    "    boundaries = {\n",
    "        'min': min_ts,\n",
    "        'train_cutoff': train_cutoff,\n",
    "        'val_cutoff': val_cutoff,\n",
    "        'max': max_ts\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nTemporal boundaries:\")\n",
    "    print(f\"  Train period: {min_ts} to {train_cutoff}\")\n",
    "    print(f\"  Val period: {train_cutoff} to {val_cutoff}\")\n",
    "    print(f\"  Test period: {val_cutoff} to {max_ts}\")\n",
    "    \n",
    "    return boundaries\n",
    "\n",
    "boundaries = get_temporal_boundaries(data['payments_events'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72af9014",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Feature Engineering\n",
    "\n",
    "### Candidate Feature List\n",
    "\n",
    "| Feature Category | Features | Source |\n",
    "|------------------|----------|--------|\n",
    "| **Demographics** | socdem_cluster, region, has_demographics | users |\n",
    "| **Retail Activity** | event_count, view_count, cart_count, unique_items, search_ratio | retail_events |\n",
    "| **Marketplace Activity** | event_count, view_count, like_count, rec_ratio | marketplace_events |\n",
    "| **Offers Activity** | event_count, redirect_count, conversion_rate | offers_events |\n",
    "| **Temporal** | recency_days, activity_span_days, events_per_day | all events |\n",
    "| **Cross-Domain** | total_events, active_domains, domain_diversity | all events |\n",
    "| **Monetary Signals** | (only for val/test - previous purchases) | payments |\n",
    "| **Embeddings** | user_embedding (avg of viewed items) | item_embeddings |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9d6f48",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Action type mappings\n",
    "RETAIL_ACTIONS = {'view': 'view', 'click': 'click', 'added-to-cart': 'cart'}\n",
    "MARKETPLACE_ACTIONS = {'view': 'view', 'click': 'click', 'clickout': 'clickout', 'like': 'like'}\n",
    "OFFERS_ACTIONS = {'seen': 'seen', 'offer_shown': 'shown', 'redirect_to_partner': 'redirect', 'like': 'like'}\n",
    "\n",
    "def create_event_features(events: pd.DataFrame, \n",
    "                          cutoff: pd.Timedelta,\n",
    "                          domain: str,\n",
    "                          action_map: dict) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Create features from event data for a specific domain.\n",
    "    Only uses events BEFORE the cutoff to prevent leakage.\n",
    "    \"\"\"\n",
    "    # Filter events before cutoff\n",
    "    events_filtered = events[events['timestamp'] < cutoff].copy()\n",
    "    \n",
    "    if len(events_filtered) == 0:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Create action flags\n",
    "    for action, name in action_map.items():\n",
    "        events_filtered[f'is_{name}'] = (events_filtered['action_type'] == action).astype(int)\n",
    "    \n",
    "    # Aggregate by user\n",
    "    agg_dict = {\n",
    "        'item_id': ['count', 'nunique'],\n",
    "        'timestamp': ['min', 'max'],\n",
    "    }\n",
    "    \n",
    "    # Add action-specific aggregations\n",
    "    for action, name in action_map.items():\n",
    "        agg_dict[f'is_{name}'] = 'sum'\n",
    "    \n",
    "    features = events_filtered.groupby('user_id').agg(agg_dict)\n",
    "    features.columns = [f'{domain}_{a}_{b}' for a, b in features.columns]\n",
    "    features = features.reset_index()\n",
    "    \n",
    "    # Rename for clarity\n",
    "    rename_map = {\n",
    "        f'{domain}_item_id_count': f'{domain}_event_count',\n",
    "        f'{domain}_item_id_nunique': f'{domain}_unique_items',\n",
    "        f'{domain}_timestamp_min': f'{domain}_first_event',\n",
    "        f'{domain}_timestamp_max': f'{domain}_last_event',\n",
    "    }\n",
    "    features = features.rename(columns=rename_map)\n",
    "    \n",
    "    # Compute derived features\n",
    "    if f'{domain}_event_count' in features.columns:\n",
    "        # Recency (days since last event to cutoff)\n",
    "        features[f'{domain}_recency_days'] = (\n",
    "            cutoff - features[f'{domain}_last_event']\n",
    "        ).dt.total_seconds() / 86400\n",
    "        \n",
    "        # Activity span\n",
    "        features[f'{domain}_activity_span'] = (\n",
    "            features[f'{domain}_last_event'] - features[f'{domain}_first_event']\n",
    "        ).dt.total_seconds() / 86400\n",
    "        \n",
    "        # Events per day\n",
    "        features[f'{domain}_events_per_day'] = (\n",
    "            features[f'{domain}_event_count'] / \n",
    "            features[f'{domain}_activity_span'].replace(0, 1)\n",
    "        )\n",
    "        \n",
    "        # Drop raw timestamp columns\n",
    "        features = features.drop(columns=[\n",
    "            f'{domain}_first_event', f'{domain}_last_event'\n",
    "        ], errors='ignore')\n",
    "    \n",
    "    return features\n",
    "\n",
    "\n",
    "def create_all_features(data: Dict[str, pd.DataFrame], \n",
    "                        cutoff: pd.Timedelta,\n",
    "                        include_embeddings: bool = True) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Create complete feature set for all users.\n",
    "    \"\"\"\n",
    "    print(f\"Creating features with cutoff: {cutoff}\")\n",
    "    \n",
    "    # Start with user demographics\n",
    "    users = data['users'][['user_id', 'socdem_cluster', 'region']].copy()\n",
    "    users['has_demographics'] = (\n",
    "        (users['socdem_cluster'] != -1) & (users['region'] != -1)\n",
    "    ).astype(int)\n",
    "    \n",
    "    print(f\"  Base users: {len(users):,}\")\n",
    "    \n",
    "    # Retail features\n",
    "    if data['retail_events'] is not None:\n",
    "        retail_feat = create_event_features(\n",
    "            data['retail_events'], cutoff, 'retail', RETAIL_ACTIONS\n",
    "        )\n",
    "        if len(retail_feat) > 0:\n",
    "            users = users.merge(retail_feat, on='user_id', how='left')\n",
    "            print(f\"  Retail features: {len(retail_feat.columns)-1} columns for {len(retail_feat):,} users\")\n",
    "    \n",
    "    # Marketplace features\n",
    "    if data['marketplace_events'] is not None:\n",
    "        mp_feat = create_event_features(\n",
    "            data['marketplace_events'], cutoff, 'marketplace', MARKETPLACE_ACTIONS\n",
    "        )\n",
    "        if len(mp_feat) > 0:\n",
    "            users = users.merge(mp_feat, on='user_id', how='left')\n",
    "            print(f\"  Marketplace features: {len(mp_feat.columns)-1} columns for {len(mp_feat):,} users\")\n",
    "    \n",
    "    # Offers features\n",
    "    if data['offers_events'] is not None:\n",
    "        offers_feat = create_event_features(\n",
    "            data['offers_events'], cutoff, 'offers', OFFERS_ACTIONS\n",
    "        )\n",
    "        if len(offers_feat) > 0:\n",
    "            users = users.merge(offers_feat, on='user_id', how='left')\n",
    "            print(f\"  Offers features: {len(offers_feat.columns)-1} columns for {len(offers_feat):,} users\")\n",
    "    \n",
    "    # Fill NaN with 0 for event features (users with no activity)\n",
    "    event_cols = [c for c in users.columns if any(\n",
    "        d in c for d in ['retail_', 'marketplace_', 'offers_']\n",
    "    )]\n",
    "    users[event_cols] = users[event_cols].fillna(0)\n",
    "    \n",
    "    # Cross-domain features\n",
    "    event_count_cols = [c for c in users.columns if c.endswith('_event_count')]\n",
    "    if event_count_cols:\n",
    "        users['total_events'] = users[event_count_cols].sum(axis=1)\n",
    "        users['active_domains'] = (users[event_count_cols] > 0).sum(axis=1)\n",
    "        \n",
    "        # Domain diversity (normalized entropy)\n",
    "        domain_counts = users[event_count_cols].values\n",
    "        domain_probs = domain_counts / (domain_counts.sum(axis=1, keepdims=True) + 1e-10)\n",
    "        entropy = -np.sum(domain_probs * np.log(domain_probs + 1e-10), axis=1)\n",
    "        max_entropy = np.log(len(event_count_cols))\n",
    "        users['domain_diversity'] = entropy / max_entropy\n",
    "    \n",
    "    # Conversion rates for each domain\n",
    "    for domain in ['retail', 'marketplace', 'offers']:\n",
    "        event_col = f'{domain}_event_count'\n",
    "        convert_col = None\n",
    "        if domain == 'retail' and f'{domain}_is_cart_sum' in users.columns:\n",
    "            convert_col = f'{domain}_is_cart_sum'\n",
    "        elif domain == 'offers' and f'{domain}_is_redirect_sum' in users.columns:\n",
    "            convert_col = f'{domain}_is_redirect_sum'\n",
    "        \n",
    "        if event_col in users.columns and convert_col in users.columns:\n",
    "            users[f'{domain}_conversion_rate'] = (\n",
    "                users[convert_col] / users[event_col].replace(0, np.nan)\n",
    "            ).fillna(0)\n",
    "    \n",
    "    print(f\"  Total features: {len(users.columns) - 1}\")\n",
    "    \n",
    "    return users\n",
    "\n",
    "\n",
    "# Create features for each split\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FEATURE ENGINEERING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "train_features = create_all_features(data, boundaries['train_cutoff'])\n",
    "val_features = create_all_features(data, boundaries['val_cutoff'])\n",
    "test_features = create_all_features(data, boundaries['max'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d00ab111",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Create Labels\n",
    "\n",
    "Label = 1 if user made payment in the label period, else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5696e44",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def create_labels(payments: pd.DataFrame, \n",
    "                  feature_cutoff: pd.Timedelta,\n",
    "                  label_end: pd.Timedelta) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Create binary labels: did user make a payment in [feature_cutoff, label_end]?\n",
    "    \"\"\"\n",
    "    # Filter payments in label window\n",
    "    label_payments = payments[\n",
    "        (payments['timestamp'] >= feature_cutoff) & \n",
    "        (payments['timestamp'] < label_end)\n",
    "    ]\n",
    "    \n",
    "    # Get unique users who converted\n",
    "    converted_users = set(label_payments['user_id'].unique())\n",
    "    \n",
    "    print(f\"  Label window: {feature_cutoff} to {label_end}\")\n",
    "    print(f\"  Converters: {len(converted_users):,}\")\n",
    "    \n",
    "    return converted_users\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CREATING LABELS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Train labels: conversions between train_cutoff and val_cutoff\n",
    "train_converters = create_labels(\n",
    "    data['payments_events'], \n",
    "    boundaries['train_cutoff'], \n",
    "    boundaries['val_cutoff']\n",
    ")\n",
    "train_features['converted'] = train_features['user_id'].isin(train_converters).astype(int)\n",
    "\n",
    "# Val labels: conversions between val_cutoff and max\n",
    "val_converters = create_labels(\n",
    "    data['payments_events'], \n",
    "    boundaries['val_cutoff'], \n",
    "    boundaries['max']\n",
    ")\n",
    "val_features['converted'] = val_features['user_id'].isin(val_converters).astype(int)\n",
    "\n",
    "# Test uses same as val for now (in production, would be future data)\n",
    "test_features['converted'] = test_features['user_id'].isin(val_converters).astype(int)\n",
    "\n",
    "# Show class distribution\n",
    "print(\"\\nClass distribution:\")\n",
    "for name, df in [('Train', train_features), ('Val', val_features), ('Test', test_features)]:\n",
    "    pos = df['converted'].sum()\n",
    "    total = len(df)\n",
    "    print(f\"  {name}: {pos:,} positive ({pos/total*100:.2f}%) / {total:,} total\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da472bd4",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Prepare Model Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbed3cd2",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def prepare_model_data(features_df: pd.DataFrame, \n",
    "                       exclude_cols: List[str] = None) -> Tuple[pd.DataFrame, pd.Series]:\n",
    "    \"\"\"\n",
    "    Prepare features and labels for modeling.\n",
    "    \"\"\"\n",
    "    if exclude_cols is None:\n",
    "        exclude_cols = ['user_id', 'converted']\n",
    "    \n",
    "    # Get feature columns\n",
    "    feature_cols = [c for c in features_df.columns if c not in exclude_cols]\n",
    "    \n",
    "    X = features_df[feature_cols].copy()\n",
    "    y = features_df['converted'].copy()\n",
    "    \n",
    "    # Handle any remaining NaN\n",
    "    X = X.fillna(0)\n",
    "    \n",
    "    # Handle infinite values\n",
    "    X = X.replace([np.inf, -np.inf], 0)\n",
    "    \n",
    "    return X, y, feature_cols\n",
    "\n",
    "\n",
    "# Prepare datasets\n",
    "X_train, y_train, feature_cols = prepare_model_data(train_features)\n",
    "X_val, y_val, _ = prepare_model_data(val_features)\n",
    "X_test, y_test, _ = prepare_model_data(test_features)\n",
    "\n",
    "print(f\"Feature columns: {len(feature_cols)}\")\n",
    "print(f\"Train: {X_train.shape}\")\n",
    "print(f\"Val: {X_val.shape}\")\n",
    "print(f\"Test: {X_test.shape}\")\n",
    "\n",
    "# Save feature columns for later\n",
    "feature_metadata = {\n",
    "    'feature_columns': feature_cols,\n",
    "    'created_at': datetime.now().isoformat()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e085872a",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Experiment Logging (Lightweight CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ee54bc",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class ExperimentLogger:\n",
    "    \"\"\"Simple CSV-based experiment logger.\"\"\"\n",
    "    \n",
    "    def __init__(self, log_path: str):\n",
    "        self.log_path = log_path\n",
    "        self.experiments = []\n",
    "        \n",
    "        if os.path.exists(log_path):\n",
    "            self.experiments = pd.read_csv(log_path).to_dict('records')\n",
    "    \n",
    "    def log(self, experiment: dict):\n",
    "        \"\"\"Log an experiment.\"\"\"\n",
    "        experiment['timestamp'] = datetime.now().isoformat()\n",
    "        self.experiments.append(experiment)\n",
    "        \n",
    "        # Save to CSV\n",
    "        pd.DataFrame(self.experiments).to_csv(self.log_path, index=False)\n",
    "    \n",
    "    def get_best(self, metric: str = 'val_pr_auc', higher_is_better: bool = True):\n",
    "        \"\"\"Get best experiment by metric.\"\"\"\n",
    "        if not self.experiments:\n",
    "            return None\n",
    "        \n",
    "        df = pd.DataFrame(self.experiments)\n",
    "        if metric not in df.columns:\n",
    "            return None\n",
    "        \n",
    "        if higher_is_better:\n",
    "            best_idx = df[metric].idxmax()\n",
    "        else:\n",
    "            best_idx = df[metric].idxmin()\n",
    "        \n",
    "        return df.iloc[best_idx].to_dict()\n",
    "\n",
    "# Initialize logger\n",
    "logger = ExperimentLogger(os.path.join(OUTPUT_DIR, 'experiments.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32649796",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Baseline Model: Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e667c6d",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def evaluate_model(model, X, y, prefix='') -> dict:\n",
    "    \"\"\"Comprehensive model evaluation.\"\"\"\n",
    "    # Predictions\n",
    "    y_pred_proba = model.predict_proba(X)[:, 1]\n",
    "    y_pred = (y_pred_proba >= 0.5).astype(int)\n",
    "    \n",
    "    # Metrics\n",
    "    metrics = {}\n",
    "    \n",
    "    # ROC-AUC\n",
    "    metrics[f'{prefix}roc_auc'] = roc_auc_score(y, y_pred_proba)\n",
    "    \n",
    "    # PR-AUC\n",
    "    precision, recall, _ = precision_recall_curve(y, y_pred_proba)\n",
    "    metrics[f'{prefix}pr_auc'] = auc(recall, precision)\n",
    "    \n",
    "    # Brier Score (calibration)\n",
    "    metrics[f'{prefix}brier'] = brier_score_loss(y, y_pred_proba)\n",
    "    \n",
    "    # Precision/Recall at various thresholds\n",
    "    for k in [0.1, 0.25, 0.5]:\n",
    "        y_pred_k = (y_pred_proba >= k).astype(int)\n",
    "        if y_pred_k.sum() > 0:\n",
    "            metrics[f'{prefix}precision_at_{k}'] = precision_score(y, y_pred_k, zero_division=0)\n",
    "            metrics[f'{prefix}recall_at_{k}'] = recall_score(y, y_pred_k, zero_division=0)\n",
    "    \n",
    "    return metrics, y_pred_proba\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"BASELINE: LOGISTIC REGRESSION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Scale features for logistic regression\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "\n",
    "# Train baseline\n",
    "baseline_model = LogisticRegression(\n",
    "    class_weight='balanced',\n",
    "    max_iter=1000,\n",
    "    random_state=RANDOM_SEED\n",
    ")\n",
    "baseline_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Evaluate\n",
    "train_metrics, _ = evaluate_model(baseline_model, X_train_scaled, y_train, 'train_')\n",
    "val_metrics, val_pred_baseline = evaluate_model(baseline_model, X_val_scaled, y_val, 'val_')\n",
    "\n",
    "print(f\"\\nBaseline Results:\")\n",
    "print(f\"  Train ROC-AUC: {train_metrics['train_roc_auc']:.4f}\")\n",
    "print(f\"  Train PR-AUC: {train_metrics['train_pr_auc']:.4f}\")\n",
    "print(f\"  Val ROC-AUC: {val_metrics['val_roc_auc']:.4f}\")\n",
    "print(f\"  Val PR-AUC: {val_metrics['val_pr_auc']:.4f}\")\n",
    "\n",
    "# Log experiment\n",
    "logger.log({\n",
    "    'model': 'LogisticRegression',\n",
    "    'params': 'class_weight=balanced',\n",
    "    **train_metrics,\n",
    "    **val_metrics\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e76e797",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Strong Model: LightGBM with Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8924e486",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def train_lgbm(params: dict, X_train, y_train, X_val, y_val) -> Tuple[lgb.Booster, dict]:\n",
    "    \"\"\"Train LightGBM with given parameters.\"\"\"\n",
    "    \n",
    "    # Create datasets\n",
    "    train_data = lgb.Dataset(X_train, label=y_train)\n",
    "    val_data = lgb.Dataset(X_val, label=y_val, reference=train_data)\n",
    "    \n",
    "    # Fixed parameters\n",
    "    fixed_params = {\n",
    "        'objective': 'binary',\n",
    "        'metric': 'auc',\n",
    "        'verbosity': -1,\n",
    "        'seed': RANDOM_SEED,\n",
    "        'is_unbalance': True,\n",
    "    }\n",
    "    \n",
    "    all_params = {**fixed_params, **params}\n",
    "    \n",
    "    # Train\n",
    "    callbacks = [lgb.early_stopping(50, verbose=False)]\n",
    "    model = lgb.train(\n",
    "        all_params,\n",
    "        train_data,\n",
    "        num_boost_round=1000,\n",
    "        valid_sets=[val_data],\n",
    "        callbacks=callbacks\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    \"\"\"Optuna objective function for hyperparameter tuning.\"\"\"\n",
    "    params = {\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 16, 256),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 12),\n",
    "        'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n",
    "        'feature_fraction': trial.suggest_float('feature_fraction', 0.5, 1.0),\n",
    "        'bagging_fraction': trial.suggest_float('bagging_fraction', 0.5, 1.0),\n",
    "        'bagging_freq': trial.suggest_int('bagging_freq', 1, 7),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 10.0, log=True),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 10.0, log=True),\n",
    "    }\n",
    "    \n",
    "    model = train_lgbm(params, X_train, y_train, X_val, y_val)\n",
    "    \n",
    "    # Evaluate on validation set\n",
    "    y_pred = model.predict(X_val)\n",
    "    pr_auc = auc(*precision_recall_curve(y_val, y_pred)[1::-1])\n",
    "    \n",
    "    return pr_auc\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"HYPERPARAMETER TUNING: LightGBM\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if OPTUNA_AVAILABLE:\n",
    "    # Bayesian optimization with Optuna\n",
    "    study = optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler(seed=RANDOM_SEED))\n",
    "    study.optimize(objective, n_trials=20, show_progress_bar=True)\n",
    "    \n",
    "    best_params = study.best_params\n",
    "    print(f\"\\nBest trial: {study.best_value:.4f}\")\n",
    "    print(f\"Best params: {best_params}\")\n",
    "else:\n",
    "    # Random search fallback\n",
    "    print(\"Using default parameters (Optuna not available)\")\n",
    "    best_params = {\n",
    "        'learning_rate': 0.05,\n",
    "        'num_leaves': 64,\n",
    "        'max_depth': 8,\n",
    "        'min_child_samples': 20,\n",
    "        'feature_fraction': 0.8,\n",
    "        'bagging_fraction': 0.8,\n",
    "        'bagging_freq': 5,\n",
    "        'reg_alpha': 0.1,\n",
    "        'reg_lambda': 0.1,\n",
    "    }\n",
    "\n",
    "# Train final model with best params\n",
    "print(\"\\nTraining final model...\")\n",
    "final_model = train_lgbm(best_params, X_train, y_train, X_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b8ffd2",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Comprehensive Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6470f3",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def comprehensive_evaluation(model, X, y, name: str, is_lgbm: bool = True):\n",
    "    \"\"\"Full evaluation suite with visualizations.\"\"\"\n",
    "    \n",
    "    # Get predictions\n",
    "    if is_lgbm:\n",
    "        y_pred_proba = model.predict(X)\n",
    "    else:\n",
    "        y_pred_proba = model.predict_proba(X)[:, 1]\n",
    "    \n",
    "    y_pred = (y_pred_proba >= 0.5).astype(int)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"EVALUATION: {name}\")\n",
    "    print('='*60)\n",
    "    \n",
    "    # Basic metrics\n",
    "    roc = roc_auc_score(y, y_pred_proba)\n",
    "    precision, recall, _ = precision_recall_curve(y, y_pred_proba)\n",
    "    pr = auc(recall, precision)\n",
    "    brier = brier_score_loss(y, y_pred_proba)\n",
    "    \n",
    "    print(f\"\\nCore Metrics:\")\n",
    "    print(f\"  ROC-AUC: {roc:.4f}\")\n",
    "    print(f\"  PR-AUC: {pr:.4f}\")\n",
    "    print(f\"  Brier Score: {brier:.4f}\")\n",
    "    \n",
    "    # Create figure for all plots\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    \n",
    "    # 1. ROC Curve\n",
    "    from sklearn.metrics import roc_curve\n",
    "    fpr, tpr, _ = roc_curve(y, y_pred_proba)\n",
    "    axes[0, 0].plot(fpr, tpr, label=f'ROC-AUC = {roc:.4f}')\n",
    "    axes[0, 0].plot([0, 1], [0, 1], 'k--', alpha=0.5)\n",
    "    axes[0, 0].set_xlabel('False Positive Rate')\n",
    "    axes[0, 0].set_ylabel('True Positive Rate')\n",
    "    axes[0, 0].set_title('ROC Curve')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Precision-Recall Curve\n",
    "    axes[0, 1].plot(recall, precision, label=f'PR-AUC = {pr:.4f}')\n",
    "    axes[0, 1].axhline(y=y.mean(), color='k', linestyle='--', label=f'Baseline = {y.mean():.4f}')\n",
    "    axes[0, 1].set_xlabel('Recall')\n",
    "    axes[0, 1].set_ylabel('Precision')\n",
    "    axes[0, 1].set_title('Precision-Recall Curve')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Calibration Plot\n",
    "    prob_true, prob_pred = calibration_curve(y, y_pred_proba, n_bins=10)\n",
    "    axes[0, 2].plot(prob_pred, prob_true, marker='o', label='Model')\n",
    "    axes[0, 2].plot([0, 1], [0, 1], 'k--', label='Perfect calibration')\n",
    "    axes[0, 2].set_xlabel('Mean Predicted Probability')\n",
    "    axes[0, 2].set_ylabel('Fraction of Positives')\n",
    "    axes[0, 2].set_title('Calibration Plot')\n",
    "    axes[0, 2].legend()\n",
    "    axes[0, 2].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. Lift by Decile\n",
    "    df_lift = pd.DataFrame({'pred': y_pred_proba, 'actual': y})\n",
    "    df_lift['decile'] = pd.qcut(df_lift['pred'], 10, labels=False, duplicates='drop')\n",
    "    lift_by_decile = df_lift.groupby('decile')['actual'].agg(['mean', 'sum', 'count'])\n",
    "    baseline_rate = y.mean()\n",
    "    lift_by_decile['lift'] = lift_by_decile['mean'] / baseline_rate\n",
    "    \n",
    "    axes[1, 0].bar(range(len(lift_by_decile)), lift_by_decile['lift'])\n",
    "    axes[1, 0].axhline(y=1, color='r', linestyle='--', label='Baseline')\n",
    "    axes[1, 0].set_xlabel('Decile (0 = lowest predicted probability)')\n",
    "    axes[1, 0].set_ylabel('Lift')\n",
    "    axes[1, 0].set_title('Lift by Decile')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    print(f\"\\nLift Analysis:\")\n",
    "    print(f\"  Top decile lift: {lift_by_decile['lift'].iloc[-1]:.2f}x\")\n",
    "    print(f\"  Top decile capture: {lift_by_decile['sum'].iloc[-1] / y.sum() * 100:.1f}% of conversions\")\n",
    "    \n",
    "    # 5. Precision/Recall at top-K\n",
    "    k_values = [0.01, 0.05, 0.1, 0.2, 0.3, 0.5]\n",
    "    precisions_at_k = []\n",
    "    recalls_at_k = []\n",
    "    \n",
    "    for k in k_values:\n",
    "        top_k_idx = np.argsort(y_pred_proba)[::-1][:int(len(y) * k)]\n",
    "        top_k_actual = y.values[top_k_idx] if hasattr(y, 'values') else y[top_k_idx]\n",
    "        precisions_at_k.append(top_k_actual.mean())\n",
    "        recalls_at_k.append(top_k_actual.sum() / y.sum())\n",
    "    \n",
    "    axes[1, 1].plot([k*100 for k in k_values], precisions_at_k, 'b-o', label='Precision')\n",
    "    axes[1, 1].plot([k*100 for k in k_values], recalls_at_k, 'g-o', label='Recall')\n",
    "    axes[1, 1].set_xlabel('Top K% of Users')\n",
    "    axes[1, 1].set_ylabel('Score')\n",
    "    axes[1, 1].set_title('Precision/Recall @ Top K%')\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    print(f\"\\nTop-K Analysis:\")\n",
    "    for i, k in enumerate(k_values):\n",
    "        print(f\"  Top {k*100:.0f}%: Precision={precisions_at_k[i]:.4f}, Recall={recalls_at_k[i]:.4f}\")\n",
    "    \n",
    "    # 6. Score Distribution\n",
    "    axes[1, 2].hist(y_pred_proba[y == 0], bins=50, alpha=0.5, label='Non-converters', density=True)\n",
    "    axes[1, 2].hist(y_pred_proba[y == 1], bins=50, alpha=0.5, label='Converters', density=True)\n",
    "    axes[1, 2].set_xlabel('Predicted Probability')\n",
    "    axes[1, 2].set_ylabel('Density')\n",
    "    axes[1, 2].set_title('Score Distribution by Class')\n",
    "    axes[1, 2].legend()\n",
    "    axes[1, 2].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    fig_path = os.path.join(DEMO_VIZ_DIR, f'evaluation_{name.lower().replace(\" \", \"_\")}.png')\n",
    "    plt.savefig(fig_path, dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(f\"\\nSaved: {fig_path}\")\n",
    "    \n",
    "    return {\n",
    "        'roc_auc': roc,\n",
    "        'pr_auc': pr,\n",
    "        'brier': brier,\n",
    "        'top_decile_lift': lift_by_decile['lift'].iloc[-1],\n",
    "        'precision_at_10pct': precisions_at_k[2],\n",
    "        'recall_at_10pct': recalls_at_k[2]\n",
    "    }\n",
    "\n",
    "\n",
    "# Evaluate on validation and test sets\n",
    "val_results = comprehensive_evaluation(final_model, X_val, y_val, \"Validation Set\")\n",
    "test_results = comprehensive_evaluation(final_model, X_test, y_test, \"Test Set\")\n",
    "\n",
    "# Log best model\n",
    "logger.log({\n",
    "    'model': 'LightGBM',\n",
    "    'params': json.dumps(best_params),\n",
    "    'val_roc_auc': val_results['roc_auc'],\n",
    "    'val_pr_auc': val_results['pr_auc'],\n",
    "    'val_brier': val_results['brier'],\n",
    "    'test_roc_auc': test_results['roc_auc'],\n",
    "    'test_pr_auc': test_results['pr_auc'],\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916dd33b",
   "metadata": {},
   "source": [
    "---\n",
    "## 10. Feature Importance & Ablation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e376468",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def analyze_feature_importance(model, feature_cols: List[str]):\n",
    "    \"\"\"Analyze feature importance and suggest ablations.\"\"\"\n",
    "    \n",
    "    # Get importance\n",
    "    importance = pd.DataFrame({\n",
    "        'feature': feature_cols,\n",
    "        'importance': model.feature_importance(importance_type='gain')\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"FEATURE IMPORTANCE\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(\"\\nTop 20 Features:\")\n",
    "    print(importance.head(20).to_string(index=False))\n",
    "    \n",
    "    # Plot\n",
    "    fig, ax = plt.subplots(figsize=(12, 10))\n",
    "    top_n = min(30, len(importance))\n",
    "    ax.barh(range(top_n), importance['importance'].values[:top_n][::-1])\n",
    "    ax.set_yticks(range(top_n))\n",
    "    ax.set_yticklabels(importance['feature'].values[:top_n][::-1])\n",
    "    ax.set_xlabel('Feature Importance (Gain)')\n",
    "    ax.set_title('Top 30 Feature Importance')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    fig_path = os.path.join(DEMO_VIZ_DIR, 'feature_importance.png')\n",
    "    plt.savefig(fig_path, dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(f\"\\nSaved: {fig_path}\")\n",
    "    \n",
    "    # Ablation suggestions\n",
    "    print(\"\\n\" + \"-\"*40)\n",
    "    print(\"ABLATION SUGGESTIONS\")\n",
    "    print(\"-\"*40)\n",
    "    \n",
    "    # Group by feature category\n",
    "    categories = {}\n",
    "    for feat in feature_cols:\n",
    "        if feat.startswith('retail_'):\n",
    "            cat = 'Retail'\n",
    "        elif feat.startswith('marketplace_'):\n",
    "            cat = 'Marketplace'\n",
    "        elif feat.startswith('offers_'):\n",
    "            cat = 'Offers'\n",
    "        elif feat in ['socdem_cluster', 'region', 'has_demographics']:\n",
    "            cat = 'Demographics'\n",
    "        else:\n",
    "            cat = 'Cross-Domain'\n",
    "        \n",
    "        if cat not in categories:\n",
    "            categories[cat] = []\n",
    "        categories[cat].append(feat)\n",
    "    \n",
    "    print(\"\\nFeature groups for ablation study:\")\n",
    "    for cat, feats in categories.items():\n",
    "        cat_importance = importance[importance['feature'].isin(feats)]['importance'].sum()\n",
    "        print(f\"  {cat}: {len(feats)} features, total importance = {cat_importance:.0f}\")\n",
    "    \n",
    "    return importance\n",
    "\n",
    "importance_df = analyze_feature_importance(final_model, feature_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bde4e69",
   "metadata": {},
   "source": [
    "---\n",
    "## 11. Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba91c3c",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def error_analysis(model, X, y, features_df, feature_cols):\n",
    "    \"\"\"Analyze prediction errors.\"\"\"\n",
    "    \n",
    "    y_pred_proba = model.predict(X)\n",
    "    y_pred = (y_pred_proba >= 0.5).astype(int)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ERROR ANALYSIS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(y, y_pred)\n",
    "    print(\"\\nConfusion Matrix (threshold=0.5):\")\n",
    "    print(f\"  TN: {cm[0,0]:,} | FP: {cm[0,1]:,}\")\n",
    "    print(f\"  FN: {cm[1,0]:,} | TP: {cm[1,1]:,}\")\n",
    "    \n",
    "    # Analyze false positives and false negatives\n",
    "    analysis_df = features_df.copy()\n",
    "    analysis_df['pred_proba'] = y_pred_proba\n",
    "    analysis_df['pred'] = y_pred\n",
    "    analysis_df['actual'] = y.values if hasattr(y, 'values') else y\n",
    "    analysis_df['error_type'] = 'Correct'\n",
    "    analysis_df.loc[(analysis_df['pred'] == 1) & (analysis_df['actual'] == 0), 'error_type'] = 'False Positive'\n",
    "    analysis_df.loc[(analysis_df['pred'] == 0) & (analysis_df['actual'] == 1), 'error_type'] = 'False Negative'\n",
    "    \n",
    "    print(\"\\nError Type Distribution:\")\n",
    "    print(analysis_df['error_type'].value_counts())\n",
    "    \n",
    "    # Analyze characteristics of errors\n",
    "    print(\"\\nFalse Positive Analysis (predicted convert but didn't):\")\n",
    "    fp_df = analysis_df[analysis_df['error_type'] == 'False Positive']\n",
    "    if len(fp_df) > 0:\n",
    "        print(f\"  Count: {len(fp_df):,}\")\n",
    "        print(f\"  Avg total_events: {fp_df['total_events'].mean():.1f}\")\n",
    "        print(f\"  Avg active_domains: {fp_df['active_domains'].mean():.2f}\")\n",
    "    \n",
    "    print(\"\\nFalse Negative Analysis (predicted no convert but did):\")\n",
    "    fn_df = analysis_df[analysis_df['error_type'] == 'False Negative']\n",
    "    if len(fn_df) > 0:\n",
    "        print(f\"  Count: {len(fn_df):,}\")\n",
    "        print(f\"  Avg total_events: {fn_df['total_events'].mean():.1f}\")\n",
    "        print(f\"  Avg active_domains: {fn_df['active_domains'].mean():.2f}\")\n",
    "    \n",
    "    # Segment-level analysis\n",
    "    print(\"\\n\" + \"-\"*40)\n",
    "    print(\"SEGMENT-LEVEL PERFORMANCE\")\n",
    "    print(\"-\"*40)\n",
    "    \n",
    "    # By activity level\n",
    "    analysis_df['activity_segment'] = pd.cut(\n",
    "        analysis_df['total_events'], \n",
    "        bins=[0, 1, 5, 20, 100, float('inf')],\n",
    "        labels=['0', '1-5', '6-20', '21-100', '100+']\n",
    "    )\n",
    "    \n",
    "    segment_perf = analysis_df.groupby('activity_segment').agg({\n",
    "        'actual': ['count', 'sum', 'mean'],\n",
    "        'pred_proba': 'mean'\n",
    "    })\n",
    "    segment_perf.columns = ['count', 'conversions', 'conv_rate', 'avg_pred']\n",
    "    print(\"\\nPerformance by Activity Level:\")\n",
    "    print(segment_perf)\n",
    "    \n",
    "    return analysis_df\n",
    "\n",
    "error_df = error_analysis(final_model, X_val, y_val, val_features, feature_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6dc4e73",
   "metadata": {},
   "source": [
    "---\n",
    "## 12. Model Packaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0295db23",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def package_model(model, feature_cols, scaler, best_params, output_dir):\n",
    "    \"\"\"Package model and artifacts for deployment.\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"MODEL PACKAGING\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Save LightGBM model\n",
    "    model_path = os.path.join(output_dir, 'final_model.txt')\n",
    "    model.save_model(model_path)\n",
    "    print(f\"  Saved model: {model_path}\")\n",
    "    \n",
    "    # Save feature metadata\n",
    "    metadata = {\n",
    "        'feature_columns': feature_cols,\n",
    "        'model_type': 'LightGBM',\n",
    "        'best_params': best_params,\n",
    "        'created_at': datetime.now().isoformat(),\n",
    "        'validation_pr_auc': val_results['pr_auc'],\n",
    "        'test_pr_auc': test_results['pr_auc']\n",
    "    }\n",
    "    \n",
    "    metadata_path = os.path.join(output_dir, 'model_metadata.json')\n",
    "    with open(metadata_path, 'w') as f:\n",
    "        json.dump(metadata, f, indent=2)\n",
    "    print(f\"  Saved metadata: {metadata_path}\")\n",
    "    \n",
    "    # Save scaler (for baseline model)\n",
    "    scaler_path = os.path.join(output_dir, 'scaler.pkl')\n",
    "    with open(scaler_path, 'wb') as f:\n",
    "        pickle.dump(scaler, f)\n",
    "    print(f\"  Saved scaler: {scaler_path}\")\n",
    "    \n",
    "    # Save feature importance\n",
    "    importance_path = os.path.join(output_dir, 'feature_importance.csv')\n",
    "    importance_df.to_csv(importance_path, index=False)\n",
    "    print(f\"  Saved importance: {importance_path}\")\n",
    "    \n",
    "    print(\"\\nModel artifacts saved successfully!\")\n",
    "    \n",
    "    return metadata\n",
    "\n",
    "metadata = package_model(final_model, feature_cols, scaler, best_params, OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd8c5a6",
   "metadata": {},
   "source": [
    "---\n",
    "## 13. Interactive Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de0d9bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_demo_prediction_function(model, feature_cols):\n",
    "    \"\"\"Create a function that predicts from a user profile.\"\"\"\n",
    "    \n",
    "    def predict_conversion(user_profile: dict) -> dict:\n",
    "        \"\"\"\n",
    "        Predict conversion probability from user profile.\n",
    "        \n",
    "        Args:\n",
    "            user_profile: dict with feature values\n",
    "        \n",
    "        Returns:\n",
    "            dict with prediction and interpretation\n",
    "        \"\"\"\n",
    "        # Create feature vector\n",
    "        features = pd.DataFrame([{col: user_profile.get(col, 0) for col in feature_cols}])\n",
    "        \n",
    "        # Predict\n",
    "        pred_proba = model.predict(features)[0]\n",
    "        \n",
    "        # Get decile\n",
    "        decile = min(9, int(pred_proba * 10))\n",
    "        \n",
    "        # Interpretation\n",
    "        if pred_proba >= 0.7:\n",
    "            risk_level = \"High\"\n",
    "            color = \"ðŸŸ¢\"\n",
    "        elif pred_proba >= 0.3:\n",
    "            risk_level = \"Medium\"\n",
    "            color = \"ðŸŸ¡\"\n",
    "        else:\n",
    "            risk_level = \"Low\"\n",
    "            color = \"ðŸ”´\"\n",
    "        \n",
    "        return {\n",
    "            'probability': pred_proba,\n",
    "            'decile': decile,\n",
    "            'risk_level': risk_level,\n",
    "            'color': color\n",
    "        }\n",
    "    \n",
    "    return predict_conversion\n",
    "\n",
    "predict_fn = create_demo_prediction_function(final_model, feature_cols)\n",
    "\n",
    "# Sample demo inputs\n",
    "demo_profiles = [\n",
    "    {\n",
    "        'name': 'Inactive User',\n",
    "        'profile': {\n",
    "            'socdem_cluster': 3,\n",
    "            'region': 5,\n",
    "            'has_demographics': 1,\n",
    "            'total_events': 0,\n",
    "            'active_domains': 0,\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        'name': 'Casual Browser',\n",
    "        'profile': {\n",
    "            'socdem_cluster': 2,\n",
    "            'region': 3,\n",
    "            'has_demographics': 1,\n",
    "            'retail_event_count': 10,\n",
    "            'retail_unique_items': 5,\n",
    "            'retail_is_view_sum': 8,\n",
    "            'retail_is_cart_sum': 0,\n",
    "            'retail_recency_days': 5,\n",
    "            'total_events': 10,\n",
    "            'active_domains': 1,\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        'name': 'Engaged Shopper',\n",
    "        'profile': {\n",
    "            'socdem_cluster': 1,\n",
    "            'region': 2,\n",
    "            'has_demographics': 1,\n",
    "            'retail_event_count': 50,\n",
    "            'retail_unique_items': 15,\n",
    "            'retail_is_view_sum': 30,\n",
    "            'retail_is_cart_sum': 5,\n",
    "            'retail_recency_days': 1,\n",
    "            'marketplace_event_count': 20,\n",
    "            'marketplace_is_view_sum': 15,\n",
    "            'marketplace_is_like_sum': 3,\n",
    "            'total_events': 70,\n",
    "            'active_domains': 2,\n",
    "            'domain_diversity': 0.8,\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        'name': 'Power User',\n",
    "        'profile': {\n",
    "            'socdem_cluster': 1,\n",
    "            'region': 1,\n",
    "            'has_demographics': 1,\n",
    "            'retail_event_count': 200,\n",
    "            'retail_unique_items': 50,\n",
    "            'retail_is_view_sum': 100,\n",
    "            'retail_is_cart_sum': 20,\n",
    "            'retail_recency_days': 0.5,\n",
    "            'marketplace_event_count': 100,\n",
    "            'marketplace_is_view_sum': 60,\n",
    "            'marketplace_is_like_sum': 10,\n",
    "            'offers_event_count': 30,\n",
    "            'offers_is_redirect_sum': 5,\n",
    "            'total_events': 330,\n",
    "            'active_domains': 3,\n",
    "            'domain_diversity': 0.95,\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DEMO PREDICTIONS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for demo in demo_profiles:\n",
    "    result = predict_fn(demo['profile'])\n",
    "    print(f\"\\n{demo['name']}:\")\n",
    "    print(f\"  {result['color']} Probability: {result['probability']:.4f}\")\n",
    "    print(f\"  Risk Level: {result['risk_level']}\")\n",
    "    print(f\"  Decile: {result['decile']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039b2ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive widget demo (if available)\n",
    "if WIDGETS_AVAILABLE:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"INTERACTIVE DEMO\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Create input widgets\n",
    "    retail_events = widgets.IntSlider(value=0, min=0, max=500, description='Retail Events:')\n",
    "    retail_carts = widgets.IntSlider(value=0, min=0, max=50, description='Cart Adds:')\n",
    "    marketplace_events = widgets.IntSlider(value=0, min=0, max=500, description='MP Events:')\n",
    "    offers_events = widgets.IntSlider(value=0, min=0, max=200, description='Offers Events:')\n",
    "    recency = widgets.FloatSlider(value=7, min=0, max=30, description='Recency (days):')\n",
    "    \n",
    "    output_widget = widgets.Output()\n",
    "    \n",
    "    def on_predict(b):\n",
    "        with output_widget:\n",
    "            output_widget.clear_output()\n",
    "            \n",
    "            profile = {\n",
    "                'retail_event_count': retail_events.value,\n",
    "                'retail_is_cart_sum': retail_carts.value,\n",
    "                'retail_is_view_sum': max(0, retail_events.value - retail_carts.value),\n",
    "                'retail_recency_days': recency.value,\n",
    "                'marketplace_event_count': marketplace_events.value,\n",
    "                'offers_event_count': offers_events.value,\n",
    "                'total_events': retail_events.value + marketplace_events.value + offers_events.value,\n",
    "                'active_domains': sum([\n",
    "                    1 if retail_events.value > 0 else 0,\n",
    "                    1 if marketplace_events.value > 0 else 0,\n",
    "                    1 if offers_events.value > 0 else 0\n",
    "                ]),\n",
    "                'has_demographics': 1,\n",
    "            }\n",
    "            \n",
    "            result = predict_fn(profile)\n",
    "            \n",
    "            # Create visualization\n",
    "            fig, ax = plt.subplots(figsize=(8, 3))\n",
    "            \n",
    "            # Probability bar\n",
    "            colors = ['#ff6b6b' if i < 3 else '#ffd93d' if i < 7 else '#6bcb77' for i in range(10)]\n",
    "            ax.barh(['Prediction'], [result['probability']], color=colors[result['decile']], height=0.5)\n",
    "            ax.axvline(x=0.5, color='black', linestyle='--', alpha=0.5, label='Threshold')\n",
    "            ax.set_xlim(0, 1)\n",
    "            ax.set_xlabel('Conversion Probability')\n",
    "            ax.set_title(f\"{result['color']} {result['risk_level']} Risk - Decile {result['decile']} - P={result['probability']:.4f}\")\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "    \n",
    "    predict_button = widgets.Button(description='Predict', button_style='primary')\n",
    "    predict_button.on_click(on_predict)\n",
    "    \n",
    "    print(\"\\nAdjust the sliders and click Predict:\")\n",
    "    display(widgets.VBox([\n",
    "        retail_events, retail_carts, marketplace_events, offers_events, recency,\n",
    "        predict_button, output_widget\n",
    "    ]))\n",
    "else:\n",
    "    print(\"\\nipywidgets not available. Run the demo profiles above for sample predictions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de4c05e",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eabb3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CROSS-DOMAIN CONVERSION PREDICTION - SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\"\"\n",
    "## Model Performance\n",
    "\n",
    "| Metric | Validation | Test |\n",
    "|--------|------------|------|\n",
    "| ROC-AUC | {val_results['roc_auc']:.4f} | {test_results['roc_auc']:.4f} |\n",
    "| PR-AUC | {val_results['pr_auc']:.4f} | {test_results['pr_auc']:.4f} |\n",
    "| Brier Score | {val_results['brier']:.4f} | {test_results['brier']:.4f} |\n",
    "| Top Decile Lift | {val_results['top_decile_lift']:.2f}x | {test_results['top_decile_lift']:.2f}x |\n",
    "| Precision@10% | {val_results['precision_at_10pct']:.4f} | {test_results['precision_at_10pct']:.4f} |\n",
    "\n",
    "## Output Files\n",
    "\n",
    "- {OUTPUT_DIR}/final_model.txt - LightGBM model\n",
    "- {OUTPUT_DIR}/model_metadata.json - Feature columns and params\n",
    "- {OUTPUT_DIR}/experiments.csv - Experiment log\n",
    "- {DEMO_VIZ_DIR}/*.png - Evaluation visualizations\n",
    "\n",
    "## Feature Summary\n",
    "\n",
    "- Total features: {len(feature_cols)}\n",
    "- Top 5 features: {', '.join(importance_df['feature'].head(5).tolist())}\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "1. **Model 3: Uplift Modeling** - Use conversion scores to build uplift model\n",
    "2. **A/B Testing** - Deploy model and measure real-world lift\n",
    "3. **Feature Iteration** - Add item embeddings from Model 1\n",
    "4. **Threshold Tuning** - Optimize threshold for business objectives\n",
    "\"\"\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"Model 2 training complete!\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "encoding": "# coding: utf-8",
   "executable": "/usr/bin/env python",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
