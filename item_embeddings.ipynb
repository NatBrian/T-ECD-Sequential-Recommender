{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08ff5cf5",
   "metadata": {},
   "source": [
    "# Model 1: Cross-Domain Item Embeddings\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook builds **unified item embeddings** that bridge items across multiple domains:\n",
    "- Retail items (`fmcg_*`)\n",
    "- Marketplace items (`nfmcg_*`)\n",
    "- Offers items (`offer_*`)\n",
    "- Receipt items (`approximate_item_id`)\n",
    "\n",
    "## Why This Matters\n",
    "\n",
    "The dataset has a critical linkage problem:\n",
    "- Only ~6% of receipt items can be directly matched to catalog items\n",
    "- 57-90% of brand_id values are missing in payments\n",
    "- Brand embeddings are corrupted and unavailable\n",
    "\n",
    "**Solution**: Learn embeddings from behavioral co-occurrence:\n",
    "- Items purchased together (same receipt) are similar\n",
    "- Items viewed together (same session) are similar\n",
    "\n",
    "## Approach\n",
    "\n",
    "1. **Extract co-occurrence edges** from receipts and sessions\n",
    "2. **Build item vocabulary** across all domains\n",
    "3. **Train contrastive embedding model** using InfoNCE loss\n",
    "4. **Evaluate** via link prediction and downstream tasks\n",
    "5. **Export** embeddings for use in other models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b5f68a",
   "metadata": {},
   "source": [
    "---\n",
    "## Configuration and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb43760",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Standard library\n",
    "import os\n",
    "import warnings\n",
    "from collections import defaultdict\n",
    "from typing import List, Tuple, Dict, Set\n",
    "\n",
    "# Data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ML Libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuration\n",
    "CLEANED_DATA_DIR = \"cleaned_data\"\n",
    "OUTPUT_DIR = \"models/item_embeddings\"\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# Model hyperparameters\n",
    "EMBEDDING_DIM = 128\n",
    "BATCH_SIZE = 1024\n",
    "LEARNING_RATE = 0.001\n",
    "NUM_EPOCHS = 20\n",
    "NEGATIVE_SAMPLES = 5\n",
    "SESSION_GAP_MINUTES = 30\n",
    "\n",
    "# Set random seeds\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "# Ensure output directory exists\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc6402c",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Load Cleaned Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369d93b1",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    \"\"\"Load all relevant cleaned datasets.\"\"\"\n",
    "    print(\"Loading cleaned data...\")\n",
    "    \n",
    "    data = {}\n",
    "    \n",
    "    # Payments Receipts - for co-purchase edges\n",
    "    path = os.path.join(CLEANED_DATA_DIR, 'payments_receipts_clean.parquet')\n",
    "    if os.path.exists(path):\n",
    "        data['receipts'] = pd.read_parquet(path)\n",
    "        print(f\"  Receipts: {len(data['receipts']):,} records\")\n",
    "    \n",
    "    # Event tables - for co-view edges\n",
    "    for name, filename in [\n",
    "        ('retail_events', 'retail_events_clean.parquet'),\n",
    "        ('marketplace_events', 'marketplace_events_clean.parquet'),\n",
    "    ]:\n",
    "        path = os.path.join(CLEANED_DATA_DIR, filename)\n",
    "        if os.path.exists(path):\n",
    "            data[name] = pd.read_parquet(path)\n",
    "            print(f\"  {name}: {len(data[name]):,} records\")\n",
    "    \n",
    "    # Item catalogs - for item metadata (optional enrichment)\n",
    "    for name, filename in [\n",
    "        ('retail_items', 'retail_items_clean.parquet'),\n",
    "        ('marketplace_items', 'marketplace_items_clean.parquet'),\n",
    "    ]:\n",
    "        path = os.path.join(CLEANED_DATA_DIR, filename)\n",
    "        if os.path.exists(path):\n",
    "            data[name] = pd.read_parquet(path)\n",
    "            print(f\"  {name}: {len(data[name]):,} records\")\n",
    "    \n",
    "    return data\n",
    "\n",
    "data = load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4afecff9",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Extract Co-Purchase Edges from Receipts\n",
    "\n",
    "Items purchased in the same transaction are similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e01680e",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def extract_copurchase_edges(receipts: pd.DataFrame, \n",
    "                              max_items_per_transaction: int = 50) -> List[Tuple[str, str]]:\n",
    "    \"\"\"\n",
    "    Extract item pairs from the same receipt (co-purchase).\n",
    "    \n",
    "    Args:\n",
    "        receipts: DataFrame with columns [transaction_hash, approximate_item_id]\n",
    "        max_items_per_transaction: Skip very large transactions (noise)\n",
    "    \n",
    "    Returns:\n",
    "        List of (item1, item2) pairs\n",
    "    \"\"\"\n",
    "    print(\"Extracting co-purchase edges from receipts...\")\n",
    "    \n",
    "    edges = []\n",
    "    skipped_transactions = 0\n",
    "    \n",
    "    # Group by transaction\n",
    "    for tx_hash, group in tqdm(receipts.groupby('transaction_hash'), \n",
    "                                desc=\"Processing transactions\"):\n",
    "        items = group['approximate_item_id'].unique().tolist()\n",
    "        \n",
    "        # Skip very large transactions (likely data issues or batch orders)\n",
    "        if len(items) > max_items_per_transaction:\n",
    "            skipped_transactions += 1\n",
    "            continue\n",
    "        \n",
    "        # Create all pairs within this transaction\n",
    "        for i in range(len(items)):\n",
    "            for j in range(i + 1, len(items)):\n",
    "                edges.append((str(items[i]), str(items[j])))\n",
    "    \n",
    "    print(f\"  Extracted {len(edges):,} co-purchase edges\")\n",
    "    print(f\"  Skipped {skipped_transactions} large transactions\")\n",
    "    \n",
    "    return edges\n",
    "\n",
    "copurchase_edges = extract_copurchase_edges(data['receipts'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779c51a0",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Extract Co-View Edges from Sessions\n",
    "\n",
    "Items viewed in the same session are related."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7186f8c",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def reconstruct_sessions(events: pd.DataFrame, \n",
    "                          gap_minutes: int = SESSION_GAP_MINUTES) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Reconstruct sessions from event stream.\n",
    "    \n",
    "    Sessions are defined as sequences of events from the same user\n",
    "    with less than `gap_minutes` between consecutive events.\n",
    "    \"\"\"\n",
    "    events = events.sort_values(['user_id', 'timestamp'])\n",
    "    \n",
    "    # Calculate time difference between consecutive events per user\n",
    "    events['time_diff'] = events.groupby('user_id')['timestamp'].diff()\n",
    "    \n",
    "    # Mark session breaks (gap > threshold OR first event per user)\n",
    "    gap_threshold = pd.Timedelta(minutes=gap_minutes)\n",
    "    events['session_break'] = (\n",
    "        events['time_diff'].isna() | \n",
    "        (events['time_diff'] > gap_threshold)\n",
    "    )\n",
    "    \n",
    "    # Assign session IDs\n",
    "    events['session_id'] = events['session_break'].cumsum()\n",
    "    \n",
    "    return events\n",
    "\n",
    "\n",
    "def extract_coview_edges(events: pd.DataFrame,\n",
    "                          max_items_per_session: int = 30,\n",
    "                          gap_minutes: int = SESSION_GAP_MINUTES) -> List[Tuple[str, str]]:\n",
    "    \"\"\"\n",
    "    Extract item pairs viewed in the same session (co-view).\n",
    "    \"\"\"\n",
    "    print(f\"Extracting co-view edges (session gap = {gap_minutes} min)...\")\n",
    "    \n",
    "    # Reconstruct sessions\n",
    "    events_with_sessions = reconstruct_sessions(events, gap_minutes)\n",
    "    \n",
    "    edges = []\n",
    "    skipped_sessions = 0\n",
    "    \n",
    "    # Group by session\n",
    "    for session_id, group in tqdm(events_with_sessions.groupby('session_id'),\n",
    "                                   desc=\"Processing sessions\"):\n",
    "        items = group['item_id'].unique().tolist()\n",
    "        \n",
    "        # Skip very long sessions (likely data issues or bots)\n",
    "        if len(items) > max_items_per_session:\n",
    "            skipped_sessions += 1\n",
    "            continue\n",
    "        \n",
    "        # Create all pairs within this session\n",
    "        for i in range(len(items)):\n",
    "            for j in range(i + 1, len(items)):\n",
    "                edges.append((str(items[i]), str(items[j])))\n",
    "    \n",
    "    print(f\"  Extracted {len(edges):,} co-view edges\")\n",
    "    print(f\"  Skipped {skipped_sessions} long sessions\")\n",
    "    \n",
    "    return edges\n",
    "\n",
    "# Extract from retail events\n",
    "retail_coview_edges = extract_coview_edges(data['retail_events'])\n",
    "\n",
    "# Extract from marketplace events\n",
    "marketplace_coview_edges = extract_coview_edges(data['marketplace_events'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac025fa3",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Build Item Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ae6fc3",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def build_vocabulary(all_edges: List[Tuple[str, str]]) -> Tuple[Dict[str, int], Dict[int, str]]:\n",
    "    \"\"\"\n",
    "    Build item vocabulary from all edges.\n",
    "    \n",
    "    Returns:\n",
    "        item_to_idx: mapping from item_id to integer index\n",
    "        idx_to_item: reverse mapping\n",
    "    \"\"\"\n",
    "    # Collect all unique items\n",
    "    all_items = set()\n",
    "    for item1, item2 in all_edges:\n",
    "        all_items.add(item1)\n",
    "        all_items.add(item2)\n",
    "    \n",
    "    # Create mappings\n",
    "    sorted_items = sorted(list(all_items))\n",
    "    item_to_idx = {item: idx for idx, item in enumerate(sorted_items)}\n",
    "    idx_to_item = {idx: item for item, idx in item_to_idx.items()}\n",
    "    \n",
    "    return item_to_idx, idx_to_item\n",
    "\n",
    "# Combine all edges\n",
    "all_edges = copurchase_edges + retail_coview_edges + marketplace_coview_edges\n",
    "print(f\"\\nTotal edges: {len(all_edges):,}\")\n",
    "\n",
    "# Build vocabulary\n",
    "item_to_idx, idx_to_item = build_vocabulary(all_edges)\n",
    "vocab_size = len(item_to_idx)\n",
    "print(f\"Vocabulary size: {vocab_size:,} unique items\")\n",
    "\n",
    "# Analyze item types\n",
    "item_types = defaultdict(int)\n",
    "for item in item_to_idx.keys():\n",
    "    if item.startswith('fmcg_'):\n",
    "        item_types['retail (fmcg)'] += 1\n",
    "    elif item.startswith('nfmcg_'):\n",
    "        item_types['marketplace (nfmcg)'] += 1\n",
    "    elif item.startswith('offer_'):\n",
    "        item_types['offers'] += 1\n",
    "    else:\n",
    "        item_types['receipt (approx)'] += 1\n",
    "\n",
    "print(\"\\nItem distribution by domain:\")\n",
    "for domain, count in sorted(item_types.items()):\n",
    "    print(f\"  {domain}: {count:,} ({count/vocab_size*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1180401",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Prepare Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90edfaad",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class ContrastiveDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for contrastive learning.\n",
    "    \n",
    "    Each sample is a (anchor, positive) pair.\n",
    "    Negatives are sampled randomly during training.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, edges: List[Tuple[str, str]], item_to_idx: Dict[str, int]):\n",
    "        self.item_to_idx = item_to_idx\n",
    "        \n",
    "        # Convert edges to indices\n",
    "        self.edge_pairs = []\n",
    "        for item1, item2 in edges:\n",
    "            if item1 in item_to_idx and item2 in item_to_idx:\n",
    "                idx1 = item_to_idx[item1]\n",
    "                idx2 = item_to_idx[item2]\n",
    "                # Add both directions for symmetry\n",
    "                self.edge_pairs.append((idx1, idx2))\n",
    "                self.edge_pairs.append((idx2, idx1))\n",
    "        \n",
    "        print(f\"Dataset created with {len(self.edge_pairs):,} training pairs\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.edge_pairs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        anchor_idx, positive_idx = self.edge_pairs[idx]\n",
    "        return torch.tensor(anchor_idx), torch.tensor(positive_idx)\n",
    "\n",
    "\n",
    "# Create dataset\n",
    "dataset = ContrastiveDataset(all_edges, item_to_idx)\n",
    "\n",
    "# Split into train/val\n",
    "train_size = int(0.9 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(\n",
    "    dataset, [train_size, val_size]\n",
    ")\n",
    "\n",
    "print(f\"Train: {len(train_dataset):,} pairs\")\n",
    "print(f\"Val: {len(val_dataset):,} pairs\")\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a5d4b66",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Define Contrastive Model\n",
    "\n",
    "We use a simple embedding model with InfoNCE (contrastive) loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f79b1278",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class ItemEmbeddingModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple item embedding model with projection head.\n",
    "    \n",
    "    Architecture:\n",
    "        item_id -> Embedding(vocab_size, embed_dim) -> ProjectionHead -> normalized embedding\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size: int, embed_dim: int = EMBEDDING_DIM):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        \n",
    "        # Projection head (improves contrastive learning)\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Linear(embed_dim, embed_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(embed_dim, embed_dim)\n",
    "        )\n",
    "        \n",
    "        # Initialize embeddings\n",
    "        nn.init.xavier_uniform_(self.embedding.weight)\n",
    "    \n",
    "    def forward(self, item_ids):\n",
    "        \"\"\"Get projected embeddings for item IDs.\"\"\"\n",
    "        x = self.embedding(item_ids)\n",
    "        x = self.projection(x)\n",
    "        # L2 normalize for cosine similarity\n",
    "        x = F.normalize(x, dim=-1)\n",
    "        return x\n",
    "    \n",
    "    def get_embeddings(self, item_ids):\n",
    "        \"\"\"Get raw embeddings (without projection) for inference.\"\"\"\n",
    "        with torch.no_grad():\n",
    "            return self.embedding(item_ids)\n",
    "\n",
    "\n",
    "def info_nce_loss(anchor_emb, positive_emb, temperature=0.1):\n",
    "    \"\"\"\n",
    "    InfoNCE contrastive loss.\n",
    "    \n",
    "    Maximizes similarity between anchor and positive,\n",
    "    minimizes similarity to other items in the batch (in-batch negatives).\n",
    "    \"\"\"\n",
    "    # Compute similarity matrix [batch_size, batch_size]\n",
    "    similarity = torch.matmul(anchor_emb, positive_emb.T) / temperature\n",
    "    \n",
    "    # Labels: diagonal elements are positive pairs\n",
    "    labels = torch.arange(similarity.size(0), device=similarity.device)\n",
    "    \n",
    "    # Cross-entropy loss (treat as classification: which positive matches anchor?)\n",
    "    loss = F.cross_entropy(similarity, labels)\n",
    "    \n",
    "    return loss\n",
    "\n",
    "\n",
    "# Create model\n",
    "model = ItemEmbeddingModel(vocab_size, EMBEDDING_DIM).to(device)\n",
    "print(f\"Model created with {sum(p.numel() for p in model.parameters()):,} parameters\")\n",
    "print(f\"Embedding dimension: {EMBEDDING_DIM}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af03376c",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b1341c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, train_loader, optimizer, device):\n",
    "    \"\"\"Train for one epoch.\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    num_batches = 0\n",
    "    \n",
    "    # Add progress bar for training batches\n",
    "    pbar = tqdm(train_loader, desc=\"Training\", leave=False)\n",
    "    for anchor_ids, positive_ids in pbar:\n",
    "        anchor_ids = anchor_ids.to(device)\n",
    "        positive_ids = positive_ids.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        anchor_emb = model(anchor_ids)\n",
    "        positive_emb = model(positive_ids)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = info_nce_loss(anchor_emb, positive_emb)\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        num_batches += 1\n",
    "        \n",
    "        # Update progress bar with current loss\n",
    "        pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "    \n",
    "    return total_loss / num_batches\n",
    "\n",
    "\n",
    "def evaluate(model, val_loader, device):\n",
    "    \"\"\"Evaluate on validation set.\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    num_batches = 0\n",
    "    \n",
    "    # Add progress bar for validation\n",
    "    pbar = tqdm(val_loader, desc=\"Validating\", leave=False)\n",
    "    with torch.no_grad():\n",
    "        for anchor_ids, positive_ids in pbar:\n",
    "            anchor_ids = anchor_ids.to(device)\n",
    "            positive_ids = positive_ids.to(device)\n",
    "            \n",
    "            anchor_emb = model(anchor_ids)\n",
    "            positive_emb = model(positive_ids)\n",
    "            \n",
    "            loss = info_nce_loss(anchor_emb, positive_emb)\n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "            \n",
    "            pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "    \n",
    "    return total_loss / num_batches\n",
    "\n",
    "\n",
    "# Training\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Train batches: {len(train_loader):,}\")\n",
    "print(f\"Val batches: {len(val_loader):,}\")\n",
    "print(f\"Epochs: {NUM_EPOCHS}\")\n",
    "print()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='min', factor=0.5, patience=2\n",
    ")\n",
    "\n",
    "history = {'train_loss': [], 'val_loss': []}\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(f\"Epoch {epoch+1}/{NUM_EPOCHS}\")\n",
    "    \n",
    "    train_loss = train_epoch(model, train_loader, optimizer, device)\n",
    "    val_loss = evaluate(model, val_loader, device)\n",
    "    \n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    \n",
    "    # Learning rate scheduling\n",
    "    old_lr = optimizer.param_groups[0]['lr']\n",
    "    scheduler.step(val_loss)\n",
    "    new_lr = optimizer.param_groups[0]['lr']\n",
    "    \n",
    "    # Save best model\n",
    "    saved_marker = \"\"\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), os.path.join(OUTPUT_DIR, 'best_model.pt'))\n",
    "        saved_marker = \"saved\"\n",
    "    \n",
    "    # Print epoch summary\n",
    "    lr_change = f\" (lr: {old_lr:.6f} â†’ {new_lr:.6f})\" if old_lr != new_lr else \"\"\n",
    "    print(f\"  Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}{lr_change}{saved_marker}\")\n",
    "    print()\n",
    "\n",
    "print(f\"Best validation loss: {best_val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "952c589b",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Training Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e80a19",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.plot(history['train_loss'], label='Train Loss', marker='o')\n",
    "ax.plot(history['val_loss'], label='Val Loss', marker='o')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.set_title('Contrastive Learning Training History')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTPUT_DIR, 'training_history.png'), dpi=150)\n",
    "plt.show()\n",
    "print(f\"Saved: {OUTPUT_DIR}/training_history.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c454c8",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Export Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8c9fd5",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def export_embeddings(model, item_to_idx, idx_to_item, output_dir):\n",
    "    \"\"\"Export trained embeddings to parquet file.\"\"\"\n",
    "    print(\"Exporting embeddings...\")\n",
    "    \n",
    "    # Load best model\n",
    "    model.load_state_dict(torch.load(os.path.join(output_dir, 'best_model.pt')))\n",
    "    model.eval()\n",
    "    \n",
    "    # Get all item indices\n",
    "    all_indices = torch.arange(len(item_to_idx), device=device)\n",
    "    \n",
    "    # Get embeddings in batches\n",
    "    embeddings_list = []\n",
    "    batch_size = 10000\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(0, len(all_indices), batch_size), desc=\"Exporting\"):\n",
    "            batch_indices = all_indices[i:i+batch_size]\n",
    "            batch_embeddings = model.get_embeddings(batch_indices)\n",
    "            embeddings_list.append(batch_embeddings.cpu().numpy())\n",
    "    \n",
    "    # Combine\n",
    "    all_embeddings = np.vstack(embeddings_list)\n",
    "    \n",
    "    # Create DataFrame\n",
    "    embedding_df = pd.DataFrame({\n",
    "        'item_id': [idx_to_item[i] for i in range(len(item_to_idx))],\n",
    "        'embedding': list(all_embeddings)\n",
    "    })\n",
    "    \n",
    "    # Add item metadata\n",
    "    def get_item_domain(item_id):\n",
    "        if item_id.startswith('fmcg_'):\n",
    "            return 'retail'\n",
    "        elif item_id.startswith('nfmcg_'):\n",
    "            return 'marketplace'\n",
    "        elif item_id.startswith('offer_'):\n",
    "            return 'offers'\n",
    "        else:\n",
    "            return 'receipt'\n",
    "    \n",
    "    embedding_df['domain'] = embedding_df['item_id'].apply(get_item_domain)\n",
    "    \n",
    "    # Save\n",
    "    output_path = os.path.join(output_dir, 'item_embeddings.parquet')\n",
    "    embedding_df.to_parquet(output_path, index=False)\n",
    "    print(f\"Saved {len(embedding_df):,} embeddings to {output_path}\")\n",
    "    \n",
    "    # Also save vocabulary mapping\n",
    "    vocab_path = os.path.join(output_dir, 'item_vocabulary.parquet')\n",
    "    vocab_df = pd.DataFrame({\n",
    "        'item_id': list(item_to_idx.keys()),\n",
    "        'index': list(item_to_idx.values())\n",
    "    })\n",
    "    vocab_df.to_parquet(vocab_path, index=False)\n",
    "    print(f\"Saved vocabulary to {vocab_path}\")\n",
    "    \n",
    "    return embedding_df\n",
    "\n",
    "embedding_df = export_embeddings(model, item_to_idx, idx_to_item, OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b70d1e",
   "metadata": {},
   "source": [
    "---\n",
    "## 10. Evaluation: Similarity Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4808c0",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def find_similar_items(query_item: str, \n",
    "                       embedding_df: pd.DataFrame, \n",
    "                       top_k: int = 10) -> pd.DataFrame:\n",
    "    \"\"\"Find most similar items to a query item.\"\"\"\n",
    "    if query_item not in item_to_idx:\n",
    "        print(f\"Item {query_item} not in vocabulary\")\n",
    "        return None\n",
    "    \n",
    "    # Get query embedding\n",
    "    query_idx = embedding_df[embedding_df['item_id'] == query_item].index[0]\n",
    "    query_emb = np.array(embedding_df.iloc[query_idx]['embedding'])\n",
    "    \n",
    "    # Compute similarities\n",
    "    all_embeddings = np.vstack(embedding_df['embedding'].values)\n",
    "    similarities = np.dot(all_embeddings, query_emb)\n",
    "    \n",
    "    # Get top-k (excluding query itself)\n",
    "    top_indices = np.argsort(similarities)[::-1][1:top_k+1]\n",
    "    \n",
    "    result = embedding_df.iloc[top_indices][['item_id', 'domain']].copy()\n",
    "    result['similarity'] = similarities[top_indices]\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "# Demonstrate with sample items from each domain\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SIMILARITY ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Find a sample item from each domain\n",
    "sample_items = {}\n",
    "for domain in ['retail', 'marketplace', 'receipt']:\n",
    "    domain_items = embedding_df[embedding_df['domain'] == domain]['item_id'].values\n",
    "    if len(domain_items) > 0:\n",
    "        sample_items[domain] = domain_items[0]\n",
    "\n",
    "for domain, item_id in sample_items.items():\n",
    "    print(f\"\\n{domain.upper()} item: {item_id}\")\n",
    "    similar = find_similar_items(item_id, embedding_df, top_k=5)\n",
    "    if similar is not None:\n",
    "        print(similar.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18dcf0b4",
   "metadata": {},
   "source": [
    "---\n",
    "## 11. Evaluation: Cross-Domain Linkage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46989c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_cross_domain_links(embedding_df: pd.DataFrame, \n",
    "                                threshold: float = 0.8) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Find cross-domain item links based on embedding similarity.\n",
    "    \n",
    "    This helps solve the receipt-to-catalog matching problem:\n",
    "    which receipt items (approximate_item_id) are similar to catalog items?\n",
    "    \"\"\"\n",
    "    print(f\"\\nAnalyzing cross-domain links (similarity threshold = {threshold})...\")\n",
    "    \n",
    "    # Get receipt items\n",
    "    receipt_items = embedding_df[embedding_df['domain'] == 'receipt']\n",
    "    catalog_items = embedding_df[embedding_df['domain'].isin(['retail', 'marketplace'])]\n",
    "    \n",
    "    print(f\"  Receipt items: {len(receipt_items):,}\")\n",
    "    print(f\"  Catalog items: {len(catalog_items):,}\")\n",
    "    \n",
    "    if len(receipt_items) == 0 or len(catalog_items) == 0:\n",
    "        print(\"  No cross-domain pairs possible\")\n",
    "        return None\n",
    "    \n",
    "    # Compute similarity matrix (sample for efficiency)\n",
    "    max_receipt = min(10000, len(receipt_items))\n",
    "    receipt_sample = receipt_items.sample(n=max_receipt, random_state=RANDOM_SEED)\n",
    "    \n",
    "    receipt_embs = np.vstack(receipt_sample['embedding'].values)\n",
    "    catalog_embs = np.vstack(catalog_items['embedding'].values)\n",
    "    \n",
    "    # Find best catalog match for each receipt item\n",
    "    links = []\n",
    "    for i, (_, row) in enumerate(tqdm(receipt_sample.iterrows(), \n",
    "                                        total=len(receipt_sample),\n",
    "                                        desc=\"Matching\")):\n",
    "        receipt_emb = np.array(row['embedding'])\n",
    "        sims = np.dot(catalog_embs, receipt_emb)\n",
    "        best_idx = np.argmax(sims)\n",
    "        best_sim = sims[best_idx]\n",
    "        \n",
    "        if best_sim >= threshold:\n",
    "            links.append({\n",
    "                'receipt_item': row['item_id'],\n",
    "                'catalog_item': catalog_items.iloc[best_idx]['item_id'],\n",
    "                'catalog_domain': catalog_items.iloc[best_idx]['domain'],\n",
    "                'similarity': best_sim\n",
    "            })\n",
    "    \n",
    "    links_df = pd.DataFrame(links)\n",
    "    print(f\"\\n  Found {len(links_df):,} high-confidence matches ({len(links_df)/max_receipt*100:.1f}%)\")\n",
    "    \n",
    "    if len(links_df) > 0:\n",
    "        print(f\"\\n  Sample matches:\")\n",
    "        print(links_df.head(10).to_string(index=False))\n",
    "    \n",
    "    return links_df\n",
    "\n",
    "cross_domain_links = analyze_cross_domain_links(embedding_df, threshold=0.7)\n",
    "\n",
    "# Save links\n",
    "if cross_domain_links is not None and len(cross_domain_links) > 0:\n",
    "    links_path = os.path.join(OUTPUT_DIR, 'cross_domain_links.parquet')\n",
    "    cross_domain_links.to_parquet(links_path, index=False)\n",
    "    print(f\"\\nSaved cross-domain links to {links_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2792a031",
   "metadata": {},
   "source": [
    "---\n",
    "## 12. Summary and Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8ef0a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CROSS-DOMAIN ITEM EMBEDDINGS - SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\"\"\n",
    "## Training Results\n",
    "\n",
    "- Vocabulary size: {vocab_size:,} unique items\n",
    "- Embedding dimension: {EMBEDDING_DIM}\n",
    "- Final validation loss: {history['val_loss'][-1]:.4f}\n",
    "- Best validation loss: {best_val_loss:.4f}\n",
    "\n",
    "## Output Files\n",
    "\n",
    "- {OUTPUT_DIR}/best_model.pt - Trained PyTorch model\n",
    "- {OUTPUT_DIR}/item_embeddings.parquet - Item embeddings ({len(embedding_df):,} items)\n",
    "- {OUTPUT_DIR}/item_vocabulary.parquet - Item ID to index mapping\n",
    "- {OUTPUT_DIR}/cross_domain_links.parquet - Receipt-to-catalog matches\n",
    "- {OUTPUT_DIR}/training_history.png - Training curves\n",
    "\n",
    "## Item Distribution\n",
    "\n",
    "\"\"\")\n",
    "\n",
    "for domain, count in embedding_df['domain'].value_counts().items():\n",
    "    print(f\"  {domain}: {count:,} ({count/len(embedding_df)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\"\"\n",
    "\n",
    "## Next Steps (Use These Embeddings In)\n",
    "\n",
    "1. **Model 2: Conversion Prediction**\n",
    "   - Average embeddings of viewed/purchased items per user\n",
    "   - Use as user representation features\n",
    "\n",
    "2. **Model 4: Session Ranking**\n",
    "   - Use embeddings for candidate retrieval (ANN)\n",
    "   - Initialize session transformer with pre-trained embeddings\n",
    "\n",
    "3. **Data Quality**\n",
    "   - Use cross_domain_links to enrich receipts with catalog metadata\n",
    "   - Enables brand/category analysis on payment data\n",
    "\"\"\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"Model 1 training complete!\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "encoding": "# coding: utf-8",
   "executable": "/usr/bin/env python",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
